{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1:\n",
    "\n",
    "Build a Support Vector Machine regressor (sklearn.svm.SVR) for the 'Calfornia Housing Prices' dataset. Try various hyperparameters, such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters).\n",
    "\n",
    "How does the best SVR predictor perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2:\n",
    "\n",
    "Try using GridSearchCV (sklearn.model_selection.GridSearchCV) and/or RandomizedSearchCV (sklearn.model_selection.RandomizedSearchCV) to search for the best hyperparameters for the SVR. \n",
    "\n",
    "Note: this will take a long time (possibly hours, depending on your hardware).\n",
    "you can set the n_jobs hyperparameter to the number of CPU cores you want to use, or -1 if you want to use all available cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3:\n",
    "\n",
    "Try adding a transformer in the preparation pipeline to select only the most important attributes. \n",
    "\n",
    "*Hint*: you can use a sklearn.feature_selection.SelectKBest feature selector that selects the top k features based on their correlation with the output target. You can use the sklearn.feature_selection.f_regression scoring function to select the most important features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1:\n",
    "\n",
    "Build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set. \n",
    "\n",
    "*Hint*: use a KNeighborsClassifier, which works quite well for this task; you just need to find good hyperparameter values (try a grid search on the weights and n_neighbors hyperparameters).\n",
    "\n",
    "Check the Scikit-Learn’s documentation for KNeighborsClassifier for more information at https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2:\n",
    "\n",
    "Tackle the Titanic dataset. A great place to start is on Kaggle. \n",
    "\n",
    "*Hint*: check out the competition notebooks at\n",
    "https://www.kaggle.com/competitions/titanic/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1:\n",
    "\n",
    "Train and fine-tune a Decision Tree for the moons dataset by following these steps:\n",
    "\n",
    "1- Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset.\n",
    "\n",
    "2- Use train_test_split() to split the dataset into a training set and a test set.\n",
    "\n",
    "3- Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes.\n",
    "\n",
    "4- Train it on the full training set using these hyperparameters, and measure your model’s performance on the test set. You should get roughly 85% to 87% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 3.2:\n",
    "\n",
    "Use ensemble methods with the same dataset. Use a RandomForestClassifier and compare its performance to the Decision Tree. Use adaBoost and Gradient Boosting to see if you can improve the performance even further.\n",
    "\n",
    "For more information about ensemble methods, check the Scikit-Learn’s documentation at https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 3.3:\n",
    "\n",
    "Build a spam classifier (a more challenging exercise):\n",
    "\n",
    "- Download examples of spam and ham from Apache SpamAssassin’s public datasets.\n",
    "\n",
    "- Unzip the datasets and familiarize yourself with the data format.\n",
    "\n",
    "- Split the datasets into a training set and a test set.\n",
    "\n",
    "- Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector that indicates the presence or absence of each possible word. For example, if all emails only ever contain four words, “Hello,” “how,” “are,” “you,” then the email “Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1] (meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word.\n",
    "\n",
    "- You may want to add hyperparameters to your preparation pipeline to control whether or not to strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with “URL,” replace all numbers with “NUMBER,” or even perform stemming (i.e., trim off word endings; there are Python libraries available to do this).\n",
    "\n",
    "- Finally, try out several classifiers and see if you can build a great spam classifier, with both high recall and high precision.\n",
    "\n",
    "\n",
    "For guidance, check out the example code in Chapter 3 from Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, at https://github.com/ageron/handson-ml3/blob/main/03_classification.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 4.1:\n",
    "\n",
    "Load the MNIST dataset and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset’s dimensionality, with an explained variance ratio of 95%. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next, evaluate the classifier on the test set. How does it compare to the previous classifier?\n",
    "\n",
    "*Hint*: refer to this example for more information about PCA: https://github.com/hkhdair/ai-ml-tute/blob/main/04.b-pca-mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 4.2:\n",
    "\n",
    "The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. Load the dataset using the sklearn.datasets.fetch_olivetti_faces() function, then split it into a training set, a validation set, and a test set (note that the dataset is already scaled between 0 and 1). Since the dataset is quite small, you probably want to use stratified sampling to ensure that there are the same number of images per person in each set. Next, cluster the images using K-Means, and ensure that you have a good number of clusters (using one of the techniques discussed in this chapter). Visualize the clusters: do you see similar faces in each cluster?\n",
    "\n",
    "For more information about the Olivetti faces dataset, check the Scikit-Learn’s documentation at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 4.3:\n",
    "\n",
    "Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set. Next, use K-Means as a dimensionality reduction tool, and train a classifier on the reduced set. Search for the number of clusters that allows the classifier to get the best performance: what performance can you reach? What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)?\n",
    "\n",
    "For guidance on the completing these exercises, check the example code in Chapter 8 from Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, at https://colab.research.google.com/github/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 5.1:\n",
    "\n",
    "Train a deep MLP on the MNIST dataset (you can load it using keras.datasets.mnist.load_data(). See if you can get over 98% precision. \n",
    "\n",
    "Try searching for the optimal learning rate by using the approach presented in chapter 10 from Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition (i.e., by growing the learning rate exponentially, plotting the loss, and finding the point where the loss shoots up).\n",
    "\n",
    "Check the chapter at: https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 6.1:\n",
    "\n",
    "Train a deep convolutional neural network on the CIFAR10 dataset, and try to achieve the highest possible accuracy. For inspiration, you can refer to this example: \n",
    "https://www.kaggle.com/code/ektasharma/simple-cifar10-cnn-keras-code-with-88-accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 7.1:\n",
    "\n",
    "Use policy gradients to solve OpenAI Gym’s LunarLander-v2 environment. You will need to install the Box2D dependencies (python3 -m pip install -U gym[box2d]).\n",
    "\n",
    "Complete solutions including this and other exercises from the book are available at\n",
    "\n",
    "https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Excercises. Make sure to save your work and submit the notebook to the LMS.\n",
    "Many thanks for your time and effort. :)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
